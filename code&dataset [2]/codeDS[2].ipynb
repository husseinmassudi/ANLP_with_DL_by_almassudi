{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67ecd120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: farasapy in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (0.0.14)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from farasapy) (2.27.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from farasapy) (4.64.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->farasapy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->farasapy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->farasapy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->farasapy) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->farasapy) (0.3.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "##Load the necessary Libraries\n",
    "!pip install farasapy\n",
    "import re\n",
    "import string\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk as nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "#Filtering out the warnings\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a23e4b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Unicode\n",
    "HAMZA = u'\\u0621'\n",
    "ALEF_MADDA = u'\\u0622'\n",
    "ALEF_HAMZA_ABOVE = u'\\u0623'\n",
    "WAW_HAMZA = u'\\u0624'\n",
    "ALEF_HAMZA_BELOW = u'\\u0625'\n",
    "YEH_HAMZA = u'\\u0626'\n",
    "ALEF = u'\\u0627'\n",
    "BEH = u'\\u0628'\n",
    "TEH_MARBUTA = u'\\u0629'\n",
    "TEH = u'\\u062a'\n",
    "THEH = u'\\u062b'\n",
    "JEEM = u'\\u062c'\n",
    "HAH = u'\\u062d'\n",
    "KHAH = u'\\u062e'\n",
    "DAL = u'\\u062f'\n",
    "THAL = u'\\u0630'\n",
    "REH = u'\\u0631'\n",
    "ZAIN = u'\\u0632'\n",
    "SEEN = u'\\u0633'\n",
    "SHEEN = u'\\u0634'\n",
    "SAD = u'\\u0635'\n",
    "DAD = u'\\u0636'\n",
    "TAH = u'\\u0637'\n",
    "ZAH = u'\\u0638'\n",
    "AIN = u'\\u0639'\n",
    "GHAIN = u'\\u063a'\n",
    "TATWEEL = u'\\u0640'\n",
    "FEH = u'\\u0641'\n",
    "QAF = u'\\u0642'\n",
    "KAF = u'\\u0643'\n",
    "LAM = u'\\u0644'\n",
    "MEEM = u'\\u0645'\n",
    "NOON = u'\\u0646'\n",
    "HEH = u'\\u0647'\n",
    "WAW = u'\\u0648'\n",
    "ALEF_MAKSURA = u'\\u0649'\n",
    "YEH = u'\\u064a'\n",
    "MADDA_ABOVE = u'\\u0653'\n",
    "HAMZA_ABOVE = u'\\u0654'\n",
    "HAMZA_BELOW = u'\\u0655'\n",
    "ZERO = u'\\u0660'\n",
    "ONE = u'\\u0661'\n",
    "TWO = u'\\u0662'\n",
    "THREE = u'\\u0663'\n",
    "FOUR = u'\\u0664'\n",
    "FIVE = u'\\u0665'\n",
    "SIX = u'\\u0666'\n",
    "SEVEN = u'\\u0667'\n",
    "EIGHT = u'\\u0668'\n",
    "NINE = u'\\u0669'\n",
    "PERCENT = u'\\u066a'\n",
    "DECIMAL = u'\\u066b'\n",
    "THOUSANDS = u'\\u066c'\n",
    "STAR = u'\\u066d'\n",
    "MINI_ALEF = u'\\u0670'\n",
    "ALEF_WASLA = u'\\u0671'\n",
    "FULL_STOP = u'\\u06d4'\n",
    "BYTE_ORDER_MARK = u'\\ufeff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b913b78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>مفتشو الأسلحة الكيمياوية يباشرون دورهم في دوما...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>الدولار قرب أدنى مستوى والدولار يتدهور</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>تم تحرير جنوبي سوريا من يد تنظيم الدولة</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>هدوء بين الجيش الإسرائيلي ومتظاهرين فلسطينيين ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>المرأة مضطهدة في عام 2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0     0  مفتشو الأسلحة الكيمياوية يباشرون دورهم في دوما...\n",
       "1     0             الدولار قرب أدنى مستوى والدولار يتدهور\n",
       "2     0            تم تحرير جنوبي سوريا من يد تنظيم الدولة\n",
       "3     0  هدوء بين الجيش الإسرائيلي ومتظاهرين فلسطينيين ...\n",
       "4     0                          المرأة مضطهدة في عام 2018"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('arabic_news2.csv')\n",
    "df = df[['comman', 'text', 'label']]\n",
    "df = df[['label', 'text']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9524795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38f4ffe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>مفتشو الأسلحة الكيمياوية يباشرون دورهم في دوما...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>الدولار قرب أدنى مستوى والدولار يتدهور</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>تم تحرير جنوبي سوريا من يد تنظيم الدولة</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>هدوء بين الجيش الإسرائيلي ومتظاهرين فلسطينيين ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>المرأة مضطهدة في عام</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0     0  مفتشو الأسلحة الكيمياوية يباشرون دورهم في دوما...\n",
       "1     0             الدولار قرب أدنى مستوى والدولار يتدهور\n",
       "2     0            تم تحرير جنوبي سوريا من يد تنظيم الدولة\n",
       "3     0  هدوء بين الجيش الإسرائيلي ومتظاهرين فلسطينيين ...\n",
       "4     0                               المرأة مضطهدة في عام"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "emoji_group =u\"\\U0001F600-\\U0001F64F\", u\"\\U0001F300-\\U0001F5FF\", u\"\\U0001F680-\\U0001F6FF\", u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"emoji_group\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "#2\n",
    "def remove_non_arabic(text):\n",
    "    return ' '.join(re.sub(u\"[^\\u0621-\\u063A\\u0640-\\u0652 ]\", \" \", text,  flags=re.UNICODE).split())\n",
    "#3\n",
    "def strip_tatweel(text):\n",
    "    return re.sub(u'[%s]' % TATWEEL, '', text)\n",
    "#4\n",
    "def remove_underscore(text):\n",
    "    return ' '.join(text.split('_'))\n",
    "#5\n",
    "def remove_extra_spaces(text):\n",
    "    return ' '.join(text.split())\n",
    "#6\n",
    "def remove_retweet_tag(text):\n",
    "    return re.compile('\\#').sub('', re.compile('rt @[a-zA-Z0-9_]+:|@[a-zA-Z0-9_]+').sub('', text).strip())\n",
    "df['C_non_ar'] = df['text'].apply(lambda x: remove_non_arabic(x))\n",
    "df['C_tatweel'] = df['C_non_ar'].apply(lambda x: strip_tatweel(x))\n",
    "df['C__underscore'] = df['C_tatweel'].apply(lambda x: remove_underscore(x))\n",
    "df['C_extra_spaces'] = df['C__underscore'].apply(lambda x: remove_extra_spaces(x))\n",
    "df['C_retweet_tag'] = df['C_extra_spaces'].apply(lambda x: remove_retweet_tag(x))\n",
    "df['C_emojis'] = df['C_retweet_tag'].apply(lambda x: remove_emojis(x))\n",
    "\n",
    "df = df[['label', 'text', 'C_emojis']]\n",
    "df = df.rename(columns={'text': 'old_text', 'C_emojis': 'text'})\n",
    "df = df[['label', 'text']]\n",
    "df.head()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57ea303c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: snowballstemmer in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3788, 6401)\n",
      "Sparse Matrix :\n",
      "   (0, 5477)\t1\n",
      "  (0, 666)\t1\n",
      "  (0, 1697)\t1\n",
      "  (0, 6069)\t1\n",
      "  (0, 3603)\t1\n",
      "  (0, 3613)\t1\n",
      "  (0, 1348)\t1\n",
      "  (1, 1208)\t1\n",
      "  (1, 4468)\t1\n",
      "  (1, 71)\t1\n",
      "  (1, 5364)\t1\n",
      "  (1, 5850)\t1\n",
      "  (1, 6096)\t1\n",
      "  (2, 3173)\t1\n",
      "  (2, 2832)\t1\n",
      "  (2, 3355)\t1\n",
      "  (2, 3872)\t1\n",
      "  (2, 6170)\t1\n",
      "  (2, 3213)\t1\n",
      "  (2, 1210)\t1\n",
      "  (3, 5769)\t1\n",
      "  (3, 1090)\t1\n",
      "  (3, 754)\t1\n",
      "  (3, 6024)\t1\n",
      "  (3, 4386)\t1\n",
      "  :\t:\n",
      "  (3784, 5100)\t1\n",
      "  (3784, 1375)\t1\n",
      "  (3784, 3795)\t1\n",
      "  (3784, 5163)\t1\n",
      "  (3784, 4334)\t1\n",
      "  (3785, 1318)\t1\n",
      "  (3785, 704)\t1\n",
      "  (3785, 1739)\t1\n",
      "  (3785, 2072)\t1\n",
      "  (3785, 940)\t1\n",
      "  (3785, 3135)\t1\n",
      "  (3785, 6386)\t1\n",
      "  (3785, 3144)\t1\n",
      "  (3786, 1362)\t1\n",
      "  (3786, 2904)\t1\n",
      "  (3786, 3682)\t1\n",
      "  (3786, 1448)\t1\n",
      "  (3786, 1062)\t1\n",
      "  (3787, 1999)\t1\n",
      "  (3787, 5759)\t1\n",
      "  (3787, 700)\t1\n",
      "  (3787, 4800)\t1\n",
      "  (3787, 3752)\t1\n",
      "  (3787, 222)\t1\n",
      "  (3787, 1130)\t1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 5477)\\t1\\n  (0, 666)\\t1\\n  (0, 1697)\\t1\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 1208)\\t1\\n  (0, 4468)\\t1\\n  (0, 71)\\t1\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 3173)\\t1\\n  (0, 2832)\\t1\\n  (0, 3355)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 5769)\\t1\\n  (0, 1090)\\t1\\n  (0, 754)\\t1\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 1807)\\t1\\n  (0, 5427)\\t1\\n  (0, 4162)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3783</th>\n",
       "      <td>(0, 1716)\\t1\\n  (0, 1483)\\t1\\n  (0, 3320)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3784</th>\n",
       "      <td>(0, 5584)\\t1\\n  (0, 5100)\\t1\\n  (0, 1375)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3785</th>\n",
       "      <td>(0, 1318)\\t1\\n  (0, 704)\\t1\\n  (0, 1739)\\t1\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3786</th>\n",
       "      <td>(0, 1362)\\t1\\n  (0, 2904)\\t1\\n  (0, 3682)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3787</th>\n",
       "      <td>(0, 1999)\\t1\\n  (0, 5759)\\t1\\n  (0, 700)\\t1\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3788 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0       (0, 5477)\\t1\\n  (0, 666)\\t1\\n  (0, 1697)\\t1\\...\n",
       "1       (0, 1208)\\t1\\n  (0, 4468)\\t1\\n  (0, 71)\\t1\\n...\n",
       "2       (0, 3173)\\t1\\n  (0, 2832)\\t1\\n  (0, 3355)\\t1...\n",
       "3       (0, 5769)\\t1\\n  (0, 1090)\\t1\\n  (0, 754)\\t1\\...\n",
       "4          (0, 1807)\\t1\\n  (0, 5427)\\t1\\n  (0, 4162)\\t1\n",
       "...                                                 ...\n",
       "3783    (0, 1716)\\t1\\n  (0, 1483)\\t1\\n  (0, 3320)\\t1...\n",
       "3784    (0, 5584)\\t1\\n  (0, 5100)\\t1\\n  (0, 1375)\\t1...\n",
       "3785    (0, 1318)\\t1\\n  (0, 704)\\t1\\n  (0, 1739)\\t1\\...\n",
       "3786    (0, 1362)\\t1\\n  (0, 2904)\\t1\\n  (0, 3682)\\t1...\n",
       "3787    (0, 1999)\\t1\\n  (0, 5759)\\t1\\n  (0, 700)\\t1\\...\n",
       "\n",
       "[3788 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "#punctuation\n",
    "import string\n",
    "string.punctuation\n",
    "def remove_punct(text):\n",
    "    \n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "df['nopunc'] = df['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "#2\n",
    "#tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "df['tokenized'] = df['nopunc'].apply(lambda x: tokenize(x))\n",
    "\n",
    "#3\n",
    "#stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_Ar = nltk.corpus.stopwords.words('arabic')\n",
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopwords_Ar]\n",
    "    return text\n",
    "\n",
    "df['nostop'] = df['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "#4\n",
    "#stemmer\n",
    "!pip install snowballstemmer\n",
    "from snowballstemmer import stemmer\n",
    "ar_stem = stemmer(\"arabic\")\n",
    "import nltk\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "df['stemmed'] = df['nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "### Create function to remove punctuation, tokenize, remove stopwords, and stem\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "     #tokens = re.split('\\W+', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords_Ar])\n",
    "    return text\n",
    "df=df[['label','text']]\n",
    "df['cleaned_text'] = df['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# To create a Count Vectorizer, we simply need to instantiate one.\n",
    "# There are special parameters we can set here when making the vectorizer, but\n",
    "# for the most basic example, it is not needed.\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "features_CountVec = vectorizer.fit_transform(df['cleaned_text'])\n",
    "print(features_CountVec.shape)\n",
    "print('Sparse Matrix :\\n', features_CountVec)\n",
    "#features_CountVec = pd.DataFrame(features_CountVec.toarray())\n",
    "features_CountVec = pd.DataFrame(features_CountVec)\n",
    "\n",
    "features_CountVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a29a417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "max_features = 10000\n",
    "embedding_dim = 100\n",
    "maxlen = 300\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "# Split the dataset into training and testing sets\n",
    "X = df['cleaned_text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Tokenize and pad the text sequences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca673896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce2f0091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 73s 4us/step\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 300, 100)          1000000   \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 296, 256)          128256    \n",
      "                                                                 \n",
      " global_max_pooling1d_3 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,144,769\n",
      "Trainable params: 1,144,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "782/782 [==============================] - 101s 126ms/step - loss: 0.4154 - accuracy: 0.7981 - auc_4: 0.8909 - val_loss: 0.3641 - val_accuracy: 0.8481 - val_auc_4: 0.9558\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 0.1966 - accuracy: 0.9273 - auc_4: 0.9756 - val_loss: 0.2606 - val_accuracy: 0.8906 - val_auc_4: 0.9607\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 97s 125ms/step - loss: 0.0818 - accuracy: 0.9742 - auc_4: 0.9948 - val_loss: 0.3736 - val_accuracy: 0.8742 - val_auc_4: 0.9537\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 97s 124ms/step - loss: 0.0295 - accuracy: 0.9924 - auc_4: 0.9989 - val_loss: 0.4594 - val_accuracy: 0.8774 - val_auc_4: 0.9451\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 0.0145 - accuracy: 0.9958 - auc_4: 0.9997 - val_loss: 0.4643 - val_accuracy: 0.8844 - val_auc_4: 0.9437\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 0.0167 - accuracy: 0.9947 - auc_4: 0.9998 - val_loss: 0.5010 - val_accuracy: 0.8802 - val_auc_4: 0.9401\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 99s 126ms/step - loss: 0.0130 - accuracy: 0.9959 - auc_4: 0.9997 - val_loss: 0.5912 - val_accuracy: 0.8843 - val_auc_4: 0.9344\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 98s 125ms/step - loss: 0.0121 - accuracy: 0.9962 - auc_4: 0.9997 - val_loss: 0.6528 - val_accuracy: 0.8855 - val_auc_4: 0.9316\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 99s 127ms/step - loss: 0.0101 - accuracy: 0.9965 - auc_4: 0.9999 - val_loss: 0.6934 - val_accuracy: 0.8856 - val_auc_4: 0.9307\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 99s 126ms/step - loss: 0.0124 - accuracy: 0.9957 - auc_4: 0.9998 - val_loss: 0.7386 - val_accuracy: 0.8722 - val_auc_4: 0.9289\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 0.7386 - accuracy: 0.8722 - auc_4: 0.9289\n",
      "Model\tAccuracy\tLoss\tAUC\n",
      "CNN\t0.87\t0.74\t0.93\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Define hyperparameters\n",
    "max_features = 10000\n",
    "embedding_dim = 100\n",
    "maxlen = 300\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Load and preprocess data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=max_features)\n",
    "X_train_pad = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test_pad = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(Conv1D(256, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy, auc = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN\\t{accuracy:.2f}\\t{loss:.2f}\\t{auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e63dc97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33c364eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 300, 100)          1000000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,125,569\n",
      "Trainable params: 1,125,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'binary_crossentropy/Cast' defined at (most recent call last):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_8720\\448892854.py\", line 16, in <cell line: 16>\n      history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 2151, in binary_crossentropy\n      y_true = tf.cast(y_true, y_pred.dtype)\nNode: 'binary_crossentropy/Cast'\nCast string to float is not supported\n\t [[{{node binary_crossentropy/Cast}}]] [Op:__inference_train_function_7754]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mAUC()])\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m---> 16\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set and print the results\u001b[39;00m\n\u001b[0;32m     19\u001b[0m loss, accuracy, auc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test_pad, y_test)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'binary_crossentropy/Cast' defined at (most recent call last):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_8720\\448892854.py\", line 16, in <cell line: 16>\n      history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 2151, in binary_crossentropy\n      y_true = tf.cast(y_true, y_pred.dtype)\nNode: 'binary_crossentropy/Cast'\nCast string to float is not supported\n\t [[{{node binary_crossentropy/Cast}}]] [Op:__inference_train_function_7754]"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set and print the results\n",
    "loss, accuracy, auc = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'LSTM\\t{accuracy:.2f}\\t{loss:.2f}\\t{auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ad7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build the BiLSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy, auc = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'BiLSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f82e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a372c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build the CNN+LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy, auc = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN + LSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN+BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build the CNN+BiLSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy, auc = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN + BiLSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdcaad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\t        Accuracy\t         Loss\t           AUC\n",
      "CNN\t        0.872240\t     0.738599\t     0.928871\n",
      "LSTM\t        0.872240\t     0.738599\t     0.928871\n",
      "CNN + LSTM\t0.872240\t     0.738599\t     0.928871\n",
      "BiLSTM\t        0.872240\t     0.738599\t     0.928871\n",
      "CNN + BiLSTM\t0.872240\t     0.738599\t     0.928871\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "loss, accuracy, auc = model_cnn.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')\n",
    "\n",
    "# LSTM\n",
    "loss, accuracy, auc = model_lstm.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'LSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')\n",
    "\n",
    "# CNN + LSTM\n",
    "loss, accuracy, auc = model_cnn_lstm.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN + LSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')\n",
    "\n",
    "# BiLSTM\n",
    "loss, accuracy, auc = model_bilstm.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'BiLSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')\n",
    "\n",
    "# CNN + BiLSTM\n",
    "loss, accuracy, auc = model_cnn_bilstm.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN + BiLSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7485edde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
