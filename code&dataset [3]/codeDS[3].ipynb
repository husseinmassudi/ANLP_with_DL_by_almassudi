{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67ecd120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: farasapy in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (0.0.14)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from farasapy) (2.27.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from farasapy) (4.64.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->farasapy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->farasapy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->farasapy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->farasapy) (1.26.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->farasapy) (0.3.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "##Load the necessary Libraries\n",
    "!pip install farasapy\n",
    "import re\n",
    "import string\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk as nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "#Filtering out the warnings\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a23e4b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Unicode\n",
    "HAMZA = u'\\u0621'\n",
    "ALEF_MADDA = u'\\u0622'\n",
    "ALEF_HAMZA_ABOVE = u'\\u0623'\n",
    "WAW_HAMZA = u'\\u0624'\n",
    "ALEF_HAMZA_BELOW = u'\\u0625'\n",
    "YEH_HAMZA = u'\\u0626'\n",
    "ALEF = u'\\u0627'\n",
    "BEH = u'\\u0628'\n",
    "TEH_MARBUTA = u'\\u0629'\n",
    "TEH = u'\\u062a'\n",
    "THEH = u'\\u062b'\n",
    "JEEM = u'\\u062c'\n",
    "HAH = u'\\u062d'\n",
    "KHAH = u'\\u062e'\n",
    "DAL = u'\\u062f'\n",
    "THAL = u'\\u0630'\n",
    "REH = u'\\u0631'\n",
    "ZAIN = u'\\u0632'\n",
    "SEEN = u'\\u0633'\n",
    "SHEEN = u'\\u0634'\n",
    "SAD = u'\\u0635'\n",
    "DAD = u'\\u0636'\n",
    "TAH = u'\\u0637'\n",
    "ZAH = u'\\u0638'\n",
    "AIN = u'\\u0639'\n",
    "GHAIN = u'\\u063a'\n",
    "TATWEEL = u'\\u0640'\n",
    "FEH = u'\\u0641'\n",
    "QAF = u'\\u0642'\n",
    "KAF = u'\\u0643'\n",
    "LAM = u'\\u0644'\n",
    "MEEM = u'\\u0645'\n",
    "NOON = u'\\u0646'\n",
    "HEH = u'\\u0647'\n",
    "WAW = u'\\u0648'\n",
    "ALEF_MAKSURA = u'\\u0649'\n",
    "YEH = u'\\u064a'\n",
    "MADDA_ABOVE = u'\\u0653'\n",
    "HAMZA_ABOVE = u'\\u0654'\n",
    "HAMZA_BELOW = u'\\u0655'\n",
    "ZERO = u'\\u0660'\n",
    "ONE = u'\\u0661'\n",
    "TWO = u'\\u0662'\n",
    "THREE = u'\\u0663'\n",
    "FOUR = u'\\u0664'\n",
    "FIVE = u'\\u0665'\n",
    "SIX = u'\\u0666'\n",
    "SEVEN = u'\\u0667'\n",
    "EIGHT = u'\\u0668'\n",
    "NINE = u'\\u0669'\n",
    "PERCENT = u'\\u066a'\n",
    "DECIMAL = u'\\u066b'\n",
    "THOUSANDS = u'\\u066c'\n",
    "STAR = u'\\u066d'\n",
    "MINI_ALEF = u'\\u0670'\n",
    "ALEF_WASLA = u'\\u0671'\n",
    "FULL_STOP = u'\\u06d4'\n",
    "BYTE_ORDER_MARK = u'\\ufeff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b913b78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>تراجع التضخم النصف سنوي في الجزائر</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>صكوك من الكويت بملياري دولار</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الحرب في سوريا: قافلة الإغاثة المرتقبة لن تتمك...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>الأساطير المحيطة بالممثلين المجوس في مصر تتحطم...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مصر: مجلس النواب يقر حصانة قضائية لضباط كبار ب...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                 تراجع التضخم النصف سنوي في الجزائر      1\n",
       "1                       صكوك من الكويت بملياري دولار      1\n",
       "2  الحرب في سوريا: قافلة الإغاثة المرتقبة لن تتمك...      0\n",
       "3  الأساطير المحيطة بالممثلين المجوس في مصر تتحطم...      1\n",
       "4  مصر: مجلس النواب يقر حصانة قضائية لضباط كبار ب...      0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('arabic_news.csv')\n",
    "df['label'] = df['label'].apply(lambda x: 1 if x == 'pos\\t\\t\\t\\t' else 0)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38f4ffe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>تراجع التضخم النصف سنوي في الجزائر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>صكوك من الكويت بملياري دولار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>الحرب في سوريا قافلة الإغاثة المرتقبة لن تتمكن...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>الأساطير المحيطة بالممثلين المجوس في مصر تتحطم...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>مصر مجلس النواب يقر حصانة قضائية لضباط كبار با...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1                 تراجع التضخم النصف سنوي في الجزائر\n",
       "1      1                       صكوك من الكويت بملياري دولار\n",
       "2      0  الحرب في سوريا قافلة الإغاثة المرتقبة لن تتمكن...\n",
       "3      1  الأساطير المحيطة بالممثلين المجوس في مصر تتحطم...\n",
       "4      0  مصر مجلس النواب يقر حصانة قضائية لضباط كبار با..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "emoji_group =u\"\\U0001F600-\\U0001F64F\", u\"\\U0001F300-\\U0001F5FF\", u\"\\U0001F680-\\U0001F6FF\", u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"emoji_group\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "#2\n",
    "def remove_non_arabic(text):\n",
    "    return ' '.join(re.sub(u\"[^\\u0621-\\u063A\\u0640-\\u0652 ]\", \" \", text,  flags=re.UNICODE).split())\n",
    "#3\n",
    "def strip_tatweel(text):\n",
    "    return re.sub(u'[%s]' % TATWEEL, '', text)\n",
    "#4\n",
    "def remove_underscore(text):\n",
    "    return ' '.join(text.split('_'))\n",
    "#5\n",
    "def remove_extra_spaces(text):\n",
    "    return ' '.join(text.split())\n",
    "#6\n",
    "def remove_retweet_tag(text):\n",
    "    return re.compile('\\#').sub('', re.compile('rt @[a-zA-Z0-9_]+:|@[a-zA-Z0-9_]+').sub('', text).strip())\n",
    "df['C_non_ar'] = df['text'].apply(lambda x: remove_non_arabic(x))\n",
    "df['C_tatweel'] = df['C_non_ar'].apply(lambda x: strip_tatweel(x))\n",
    "df['C__underscore'] = df['C_tatweel'].apply(lambda x: remove_underscore(x))\n",
    "df['C_extra_spaces'] = df['C__underscore'].apply(lambda x: remove_extra_spaces(x))\n",
    "df['C_retweet_tag'] = df['C_extra_spaces'].apply(lambda x: remove_retweet_tag(x))\n",
    "df['C_emojis'] = df['C_retweet_tag'].apply(lambda x: remove_emojis(x))\n",
    "\n",
    "df = df[['label', 'text', 'C_emojis']]\n",
    "df = df.rename(columns={'text': 'old_text', 'C_emojis': 'text'})\n",
    "df = df[['label', 'text']]\n",
    "df.head()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57ea303c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: snowballstemmer in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4533, 7961)\n",
      "Sparse Matrix :\n",
      "   (0, 3473)\t1\n",
      "  (0, 1161)\t1\n",
      "  (0, 2340)\t1\n",
      "  (0, 4697)\t1\n",
      "  (0, 1255)\t1\n",
      "  (1, 4922)\t1\n",
      "  (1, 1976)\t1\n",
      "  (1, 3119)\t1\n",
      "  (2, 1318)\t1\n",
      "  (2, 4713)\t1\n",
      "  (2, 5381)\t1\n",
      "  (2, 920)\t1\n",
      "  (2, 2120)\t1\n",
      "  (2, 3333)\t1\n",
      "  (2, 4357)\t1\n",
      "  (2, 1832)\t1\n",
      "  (2, 1638)\t1\n",
      "  (3, 786)\t1\n",
      "  (3, 2087)\t1\n",
      "  (3, 2789)\t1\n",
      "  (3, 2070)\t1\n",
      "  (3, 6584)\t1\n",
      "  (3, 3291)\t1\n",
      "  (3, 7650)\t1\n",
      "  (3, 2010)\t1\n",
      "  :\t:\n",
      "  (4529, 3223)\t1\n",
      "  (4529, 7071)\t1\n",
      "  (4529, 992)\t1\n",
      "  (4530, 1756)\t1\n",
      "  (4530, 3504)\t1\n",
      "  (4530, 2308)\t1\n",
      "  (4530, 7780)\t1\n",
      "  (4530, 3397)\t1\n",
      "  (4530, 7408)\t1\n",
      "  (4530, 4053)\t1\n",
      "  (4530, 4354)\t1\n",
      "  (4531, 6584)\t1\n",
      "  (4531, 5430)\t1\n",
      "  (4531, 4825)\t1\n",
      "  (4531, 2338)\t1\n",
      "  (4531, 209)\t1\n",
      "  (4531, 5187)\t1\n",
      "  (4531, 4814)\t1\n",
      "  (4532, 2345)\t1\n",
      "  (4532, 1722)\t1\n",
      "  (4532, 6734)\t1\n",
      "  (4532, 7756)\t1\n",
      "  (4532, 2944)\t1\n",
      "  (4532, 6156)\t1\n",
      "  (4532, 1649)\t1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 3473)\\t1\\n  (0, 1161)\\t1\\n  (0, 2340)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 4922)\\t1\\n  (0, 1976)\\t1\\n  (0, 3119)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 1318)\\t1\\n  (0, 4713)\\t1\\n  (0, 5381)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 786)\\t1\\n  (0, 2087)\\t1\\n  (0, 2789)\\t1\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 6584)\\t1\\n  (0, 6360)\\t1\\n  (0, 2367)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4528</th>\n",
       "      <td>(0, 1318)\\t1\\n  (0, 2427)\\t1\\n  (0, 7019)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4529</th>\n",
       "      <td>(0, 2461)\\t1\\n  (0, 1756)\\t1\\n  (0, 6889)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4530</th>\n",
       "      <td>(0, 1756)\\t1\\n  (0, 3504)\\t1\\n  (0, 2308)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4531</th>\n",
       "      <td>(0, 6584)\\t1\\n  (0, 5430)\\t1\\n  (0, 4825)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4532</th>\n",
       "      <td>(0, 2345)\\t1\\n  (0, 1722)\\t1\\n  (0, 6734)\\t1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4533 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0       (0, 3473)\\t1\\n  (0, 1161)\\t1\\n  (0, 2340)\\t1...\n",
       "1          (0, 4922)\\t1\\n  (0, 1976)\\t1\\n  (0, 3119)\\t1\n",
       "2       (0, 1318)\\t1\\n  (0, 4713)\\t1\\n  (0, 5381)\\t1...\n",
       "3       (0, 786)\\t1\\n  (0, 2087)\\t1\\n  (0, 2789)\\t1\\...\n",
       "4       (0, 6584)\\t1\\n  (0, 6360)\\t1\\n  (0, 2367)\\t1...\n",
       "...                                                 ...\n",
       "4528    (0, 1318)\\t1\\n  (0, 2427)\\t1\\n  (0, 7019)\\t1...\n",
       "4529    (0, 2461)\\t1\\n  (0, 1756)\\t1\\n  (0, 6889)\\t1...\n",
       "4530    (0, 1756)\\t1\\n  (0, 3504)\\t1\\n  (0, 2308)\\t1...\n",
       "4531    (0, 6584)\\t1\\n  (0, 5430)\\t1\\n  (0, 4825)\\t1...\n",
       "4532    (0, 2345)\\t1\\n  (0, 1722)\\t1\\n  (0, 6734)\\t1...\n",
       "\n",
       "[4533 rows x 1 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "#punctuation\n",
    "import string\n",
    "string.punctuation\n",
    "def remove_punct(text):\n",
    "    \n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "df['nopunc'] = df['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "#2\n",
    "#tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "df['tokenized'] = df['nopunc'].apply(lambda x: tokenize(x))\n",
    "\n",
    "#3\n",
    "#stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_Ar = nltk.corpus.stopwords.words('arabic')\n",
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopwords_Ar]\n",
    "    return text\n",
    "\n",
    "df['nostop'] = df['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "#4\n",
    "#stemmer\n",
    "!pip install snowballstemmer\n",
    "from snowballstemmer import stemmer\n",
    "ar_stem = stemmer(\"arabic\")\n",
    "import nltk\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "df['stemmed'] = df['nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "### Create function to remove punctuation, tokenize, remove stopwords, and stem\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "     #tokens = re.split('\\W+', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords_Ar])\n",
    "    return text\n",
    "df=df[['label','text']]\n",
    "df['cleaned_text'] = df['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# To create a Count Vectorizer, we simply need to instantiate one.\n",
    "# There are special parameters we can set here when making the vectorizer, but\n",
    "# for the most basic example, it is not needed.\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "features_CountVec = vectorizer.fit_transform(df['cleaned_text'])\n",
    "print(features_CountVec.shape)\n",
    "print('Sparse Matrix :\\n', features_CountVec)\n",
    "#features_CountVec = pd.DataFrame(features_CountVec.toarray())\n",
    "features_CountVec = pd.DataFrame(features_CountVec)\n",
    "\n",
    "features_CountVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a29a417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "max_features = 10000\n",
    "embedding_dim = 100\n",
    "maxlen = 300\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "# Split the dataset into training and testing sets\n",
    "X = df['cleaned_text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Tokenize and pad the text sequences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca673896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce2f0091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 300, 100)          1000000   \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 296, 256)          128256    \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,144,769\n",
      "Trainable params: 1,144,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 15s 110ms/step - loss: 0.5479 - accuracy: 0.7683 - auc_7: 0.5171 - val_loss: 0.5335 - val_accuracy: 0.7696 - val_auc_7: 0.5885\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 12s 106ms/step - loss: 0.4608 - accuracy: 0.7763 - auc_7: 0.7606 - val_loss: 0.5991 - val_accuracy: 0.7574 - val_auc_7: 0.5253\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 12s 105ms/step - loss: 0.2611 - accuracy: 0.8847 - auc_7: 0.9426 - val_loss: 0.9162 - val_accuracy: 0.6163 - val_auc_7: 0.5103\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 12s 104ms/step - loss: 0.1451 - accuracy: 0.9437 - auc_7: 0.9808 - val_loss: 1.2749 - val_accuracy: 0.6031 - val_auc_7: 0.4969\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 12s 106ms/step - loss: 0.0944 - accuracy: 0.9653 - auc_7: 0.9907 - val_loss: 1.2625 - val_accuracy: 0.6362 - val_auc_7: 0.4909\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 12s 106ms/step - loss: 0.0570 - accuracy: 0.9812 - auc_7: 0.9961 - val_loss: 1.5276 - val_accuracy: 0.6130 - val_auc_7: 0.5078\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 12s 103ms/step - loss: 0.0382 - accuracy: 0.9868 - auc_7: 0.9984 - val_loss: 1.7082 - val_accuracy: 0.6097 - val_auc_7: 0.4895\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 12s 104ms/step - loss: 0.0281 - accuracy: 0.9892 - auc_7: 0.9992 - val_loss: 1.8990 - val_accuracy: 0.6108 - val_auc_7: 0.4920\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 12s 106ms/step - loss: 0.0172 - accuracy: 0.9926 - auc_7: 0.9998 - val_loss: 2.1088 - val_accuracy: 0.6009 - val_auc_7: 0.4917\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 12s 106ms/step - loss: 0.0171 - accuracy: 0.9937 - auc_7: 0.9995 - val_loss: 2.2372 - val_accuracy: 0.6207 - val_auc_7: 0.4985\n",
      "29/29 [==============================] - 1s 27ms/step - loss: 2.2372 - accuracy: 0.6207 - auc_7: 0.4985\n",
      "Model\tAccuracy\tLoss\tAUC\n",
      "CNN\t0.62\t2.24\t0.50\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(Conv1D(256, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy, auc = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN\\t{accuracy:.2f}\\t{loss:.2f}\\t{auc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e63dc97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "33c364eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 300, 100)          1000000   \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,125,569\n",
      "Trainable params: 1,125,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 86s 687ms/step - loss: 0.5600 - accuracy: 0.7703 - auc_8: 0.4808 - val_loss: 0.5402 - val_accuracy: 0.7696 - val_auc_8: 0.4744\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 84s 731ms/step - loss: 0.5308 - accuracy: 0.7747 - auc_8: 0.5703 - val_loss: 0.5495 - val_accuracy: 0.7696 - val_auc_8: 0.5162\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 83s 725ms/step - loss: 0.3920 - accuracy: 0.8127 - auc_8: 0.8447 - val_loss: 0.7174 - val_accuracy: 0.6714 - val_auc_8: 0.4926\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 78s 684ms/step - loss: 0.2435 - accuracy: 0.8963 - auc_8: 0.9463 - val_loss: 0.9684 - val_accuracy: 0.6626 - val_auc_8: 0.4955\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 85s 739ms/step - loss: 0.1660 - accuracy: 0.9319 - auc_8: 0.9752 - val_loss: 1.2432 - val_accuracy: 0.6803 - val_auc_8: 0.4936\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 55s 477ms/step - loss: 0.1185 - accuracy: 0.9531 - auc_8: 0.9879 - val_loss: 1.5330 - val_accuracy: 0.6461 - val_auc_8: 0.4908\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 57s 504ms/step - loss: 0.0840 - accuracy: 0.9636 - auc_8: 0.9940 - val_loss: 1.7957 - val_accuracy: 0.6417 - val_auc_8: 0.4821\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 48s 419ms/step - loss: 0.0681 - accuracy: 0.9699 - auc_8: 0.9954 - val_loss: 2.0180 - val_accuracy: 0.6593 - val_auc_8: 0.4898\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 60s 524ms/step - loss: 0.0504 - accuracy: 0.9779 - auc_8: 0.9978 - val_loss: 2.2836 - val_accuracy: 0.6483 - val_auc_8: 0.4926\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 55s 480ms/step - loss: 0.0441 - accuracy: 0.9810 - auc_8: 0.9981 - val_loss: 2.0931 - val_accuracy: 0.6472 - val_auc_8: 0.5050\n",
      "29/29 [==============================] - 3s 116ms/step - loss: 2.0931 - accuracy: 0.6472 - auc_8: 0.5050\n",
      "Model\tAccuracy\tLoss\tAUC\n",
      "LSTM\t0.65\t2.09\t0.50\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set and print the results\n",
    "loss, accuracy, auc = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'LSTM\\t{accuracy:.2f}\\t{loss:.2f}\\t{auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4ad7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df84bd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 300, 100)          1000000   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 128)              84480     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,092,801\n",
      "Trainable params: 1,092,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 51s 389ms/step - loss: 0.5519 - accuracy: 0.7714 - auc_9: 0.4952 - val_loss: 0.5410 - val_accuracy: 0.7696 - val_auc_9: 0.5416\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 44s 384ms/step - loss: 0.5000 - accuracy: 0.7755 - auc_9: 0.6765 - val_loss: 0.5753 - val_accuracy: 0.7585 - val_auc_9: 0.5347\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 44s 383ms/step - loss: 0.3400 - accuracy: 0.8445 - auc_9: 0.8889 - val_loss: 0.7881 - val_accuracy: 0.6637 - val_auc_9: 0.5031\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 44s 383ms/step - loss: 0.2205 - accuracy: 0.9054 - auc_9: 0.9561 - val_loss: 0.9581 - val_accuracy: 0.6450 - val_auc_9: 0.4971\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 44s 385ms/step - loss: 0.1569 - accuracy: 0.9388 - auc_9: 0.9773 - val_loss: 1.3295 - val_accuracy: 0.6516 - val_auc_9: 0.4974\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 44s 383ms/step - loss: 0.1125 - accuracy: 0.9573 - auc_9: 0.9878 - val_loss: 1.6030 - val_accuracy: 0.6681 - val_auc_9: 0.4901\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 44s 382ms/step - loss: 0.0960 - accuracy: 0.9600 - auc_9: 0.9920 - val_loss: 1.6529 - val_accuracy: 0.6351 - val_auc_9: 0.4908\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 44s 383ms/step - loss: 0.0701 - accuracy: 0.9697 - auc_9: 0.9958 - val_loss: 1.9463 - val_accuracy: 0.6560 - val_auc_9: 0.4896\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 43s 380ms/step - loss: 0.0553 - accuracy: 0.9755 - auc_9: 0.9974 - val_loss: 2.3463 - val_accuracy: 0.6329 - val_auc_9: 0.4966\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 43s 377ms/step - loss: 0.0489 - accuracy: 0.9768 - auc_9: 0.9976 - val_loss: 2.3278 - val_accuracy: 0.6295 - val_auc_9: 0.4984\n",
      "29/29 [==============================] - 1s 42ms/step - loss: 2.3278 - accuracy: 0.6295 - auc_9: 0.4984\n",
      "Model\tAccuracy\tLoss\tAUC\n",
      "BiLSTM\t0.629548\t2.327836\t0.498382\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build the BiLSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy, auc = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'BiLSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f82e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a372c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 300, 100)          1000000   \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 298, 128)          38528     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 149, 128)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 149, 128)          0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,092,161\n",
      "Trainable params: 1,092,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 17s 120ms/step - loss: 0.5532 - accuracy: 0.7730 - auc_10: 0.5007 - val_loss: 0.5496 - val_accuracy: 0.7696 - val_auc_10: 0.5648\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 13s 114ms/step - loss: 0.4795 - accuracy: 0.7730 - auc_10: 0.7267 - val_loss: 0.5965 - val_accuracy: 0.7696 - val_auc_10: 0.5032\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 13s 114ms/step - loss: 0.2962 - accuracy: 0.8605 - auc_10: 0.9211 - val_loss: 0.8796 - val_accuracy: 0.6351 - val_auc_10: 0.4880\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 13s 115ms/step - loss: 0.1787 - accuracy: 0.9327 - auc_10: 0.9704 - val_loss: 1.1336 - val_accuracy: 0.5954 - val_auc_10: 0.4827\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 13s 114ms/step - loss: 0.1100 - accuracy: 0.9628 - auc_10: 0.9865 - val_loss: 1.2913 - val_accuracy: 0.6428 - val_auc_10: 0.4854\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 13s 114ms/step - loss: 0.0871 - accuracy: 0.9688 - auc_10: 0.9919 - val_loss: 1.4839 - val_accuracy: 0.6604 - val_auc_10: 0.4835\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 13s 115ms/step - loss: 0.0585 - accuracy: 0.9790 - auc_10: 0.9970 - val_loss: 1.5617 - val_accuracy: 0.6373 - val_auc_10: 0.4771\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 13s 115ms/step - loss: 0.0386 - accuracy: 0.9848 - auc_10: 0.9988 - val_loss: 1.9963 - val_accuracy: 0.6461 - val_auc_10: 0.4856\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 13s 115ms/step - loss: 0.0283 - accuracy: 0.9862 - auc_10: 0.9993 - val_loss: 2.2630 - val_accuracy: 0.6428 - val_auc_10: 0.4925\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 13s 114ms/step - loss: 0.0246 - accuracy: 0.9898 - auc_10: 0.9995 - val_loss: 2.7247 - val_accuracy: 0.6295 - val_auc_10: 0.4777\n",
      "29/29 [==============================] - 1s 31ms/step - loss: 2.7247 - accuracy: 0.6295 - auc_10: 0.4777\n",
      "Model\tAccuracy\tLoss\tAUC\n",
      "CNN + LSTM\t0.629548\t2.724747\t0.477674\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build the CNN+LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy, auc = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN + LSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13c2e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN+BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65f1324b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 300, 100)          1000000   \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 298, 128)          38528     \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 149, 128)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 149, 128)          0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,145,665\n",
      "Trainable params: 1,145,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 22s 153ms/step - loss: 0.5509 - accuracy: 0.7714 - auc_11: 0.5072 - val_loss: 0.5375 - val_accuracy: 0.7696 - val_auc_11: 0.5681\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 16s 140ms/step - loss: 0.4798 - accuracy: 0.7772 - auc_11: 0.7197 - val_loss: 0.6311 - val_accuracy: 0.7574 - val_auc_11: 0.5119\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 16s 143ms/step - loss: 0.3018 - accuracy: 0.8618 - auc_11: 0.9160 - val_loss: 0.8279 - val_accuracy: 0.6494 - val_auc_11: 0.4926\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 16s 142ms/step - loss: 0.1743 - accuracy: 0.9322 - auc_11: 0.9725 - val_loss: 1.2463 - val_accuracy: 0.6373 - val_auc_11: 0.4908\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 16s 142ms/step - loss: 0.1025 - accuracy: 0.9655 - auc_11: 0.9895 - val_loss: 1.2537 - val_accuracy: 0.6582 - val_auc_11: 0.4864\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 16s 141ms/step - loss: 0.0695 - accuracy: 0.9746 - auc_11: 0.9945 - val_loss: 1.8770 - val_accuracy: 0.6461 - val_auc_11: 0.4886\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 16s 141ms/step - loss: 0.0464 - accuracy: 0.9829 - auc_11: 0.9980 - val_loss: 1.9015 - val_accuracy: 0.6549 - val_auc_11: 0.4891\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 16s 141ms/step - loss: 0.0355 - accuracy: 0.9843 - auc_11: 0.9990 - val_loss: 2.0097 - val_accuracy: 0.6196 - val_auc_11: 0.4852\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 16s 141ms/step - loss: 0.0270 - accuracy: 0.9876 - auc_11: 0.9988 - val_loss: 2.6632 - val_accuracy: 0.6803 - val_auc_11: 0.4989\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 16s 141ms/step - loss: 0.0236 - accuracy: 0.9898 - auc_11: 0.9990 - val_loss: 3.0495 - val_accuracy: 0.6781 - val_auc_11: 0.4880\n",
      "29/29 [==============================] - 1s 40ms/step - loss: 3.0495 - accuracy: 0.6781 - auc_11: 0.4880\n",
      "Model\tAccuracy\tLoss\tAUC\n",
      "CNN + BiLSTM\t0.678060\t3.049469\t0.488042\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build the CNN+BiLSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy, auc = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN + BiLSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cdcaad73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# CNN\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loss, accuracy, auc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cnn\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(X_test_pad, y_test)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_cnn' is not defined"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "loss, accuracy, auc = model_cnn.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')\n",
    "\n",
    "# LSTM\n",
    "loss, accuracy, auc = model_lstm.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'LSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')\n",
    "\n",
    "# CNN + LSTM\n",
    "loss, accuracy, auc = model_cnn_lstm.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN + LSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')\n",
    "\n",
    "# BiLSTM\n",
    "loss, accuracy, auc = model_bilstm.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'BiLSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')\n",
    "\n",
    "# CNN + BiLSTM\n",
    "loss, accuracy, auc = model_cnn_bilstm.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN + BiLSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
