{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ecd120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: farasapy in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (0.0.14)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from farasapy) (2.27.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from farasapy) (4.64.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->farasapy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->farasapy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->farasapy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->farasapy) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->farasapy) (0.3.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "##Load the necessary Libraries\n",
    "!pip install farasapy\n",
    "import re\n",
    "import string\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk as nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "#Filtering out the warnings\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a23e4b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Unicode\n",
    "HAMZA = u'\\u0621'\n",
    "ALEF_MADDA = u'\\u0622'\n",
    "ALEF_HAMZA_ABOVE = u'\\u0623'\n",
    "WAW_HAMZA = u'\\u0624'\n",
    "ALEF_HAMZA_BELOW = u'\\u0625'\n",
    "YEH_HAMZA = u'\\u0626'\n",
    "ALEF = u'\\u0627'\n",
    "BEH = u'\\u0628'\n",
    "TEH_MARBUTA = u'\\u0629'\n",
    "TEH = u'\\u062a'\n",
    "THEH = u'\\u062b'\n",
    "JEEM = u'\\u062c'\n",
    "HAH = u'\\u062d'\n",
    "KHAH = u'\\u062e'\n",
    "DAL = u'\\u062f'\n",
    "THAL = u'\\u0630'\n",
    "REH = u'\\u0631'\n",
    "ZAIN = u'\\u0632'\n",
    "SEEN = u'\\u0633'\n",
    "SHEEN = u'\\u0634'\n",
    "SAD = u'\\u0635'\n",
    "DAD = u'\\u0636'\n",
    "TAH = u'\\u0637'\n",
    "ZAH = u'\\u0638'\n",
    "AIN = u'\\u0639'\n",
    "GHAIN = u'\\u063a'\n",
    "TATWEEL = u'\\u0640'\n",
    "FEH = u'\\u0641'\n",
    "QAF = u'\\u0642'\n",
    "KAF = u'\\u0643'\n",
    "LAM = u'\\u0644'\n",
    "MEEM = u'\\u0645'\n",
    "NOON = u'\\u0646'\n",
    "HEH = u'\\u0647'\n",
    "WAW = u'\\u0648'\n",
    "ALEF_MAKSURA = u'\\u0649'\n",
    "YEH = u'\\u064a'\n",
    "MADDA_ABOVE = u'\\u0653'\n",
    "HAMZA_ABOVE = u'\\u0654'\n",
    "HAMZA_BELOW = u'\\u0655'\n",
    "ZERO = u'\\u0660'\n",
    "ONE = u'\\u0661'\n",
    "TWO = u'\\u0662'\n",
    "THREE = u'\\u0663'\n",
    "FOUR = u'\\u0664'\n",
    "FIVE = u'\\u0665'\n",
    "SIX = u'\\u0666'\n",
    "SEVEN = u'\\u0667'\n",
    "EIGHT = u'\\u0668'\n",
    "NINE = u'\\u0669'\n",
    "PERCENT = u'\\u066a'\n",
    "DECIMAL = u'\\u066b'\n",
    "THOUSANDS = u'\\u066c'\n",
    "STAR = u'\\u066d'\n",
    "MINI_ALEF = u'\\u0670'\n",
    "ALEF_WASLA = u'\\u0671'\n",
    "FULL_STOP = u'\\u06d4'\n",
    "BYTE_ORDER_MARK = u'\\ufeff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b913b78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>من الخير نفسه 💛</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>#زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>#الاتحاد_النصر لاتحسبونا نسينا يالطواقي ولانبي...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...\n",
       "1      1                                    من الخير نفسه 💛\n",
       "2      1  #زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...\n",
       "3      1  الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...\n",
       "4      1  #الاتحاد_النصر لاتحسبونا نسينا يالطواقي ولانبي..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_tweets.tsv', delimiter='\\t')\n",
    "df.columns = ['label', 'text']\n",
    "df['label'] = df['label'].apply(lambda x: 1 if x == 'pos' else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38f4ffe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>وفي النهاية لن يبقى معك آحدإلا من رأى الجمال ف...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>من الخير نفسه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>زلزل الملعب نصرنا بيلعب كن عالي الهمه ولا ترضى...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>الشيء الوحيد الذي وصلوا فيه للعالمية هو المسيا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>الاتحاد النصر لاتحسبونا نسينا يالطواقي ولانبيك...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  وفي النهاية لن يبقى معك آحدإلا من رأى الجمال ف...\n",
       "1      1                                      من الخير نفسه\n",
       "2      1  زلزل الملعب نصرنا بيلعب كن عالي الهمه ولا ترضى...\n",
       "3      1  الشيء الوحيد الذي وصلوا فيه للعالمية هو المسيا...\n",
       "4      1  الاتحاد النصر لاتحسبونا نسينا يالطواقي ولانبيك..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "emoji_group =u\"\\U0001F600-\\U0001F64F\", u\"\\U0001F300-\\U0001F5FF\", u\"\\U0001F680-\\U0001F6FF\", u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"emoji_group\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "#2\n",
    "def remove_non_arabic(text):\n",
    "    return ' '.join(re.sub(u\"[^\\u0621-\\u063A\\u0640-\\u0652 ]\", \" \", text,  flags=re.UNICODE).split())\n",
    "#3\n",
    "def strip_tatweel(text):\n",
    "    return re.sub(u'[%s]' % TATWEEL, '', text)\n",
    "#4\n",
    "def remove_underscore(text):\n",
    "    return ' '.join(text.split('_'))\n",
    "#5\n",
    "def remove_extra_spaces(text):\n",
    "    return ' '.join(text.split())\n",
    "#6\n",
    "def remove_retweet_tag(text):\n",
    "    return re.compile('\\#').sub('', re.compile('rt @[a-zA-Z0-9_]+:|@[a-zA-Z0-9_]+').sub('', text).strip())\n",
    "df['C_non_ar'] = df['text'].apply(lambda x: remove_non_arabic(x))\n",
    "df['C_tatweel'] = df['C_non_ar'].apply(lambda x: strip_tatweel(x))\n",
    "df['C__underscore'] = df['C_tatweel'].apply(lambda x: remove_underscore(x))\n",
    "df['C_extra_spaces'] = df['C__underscore'].apply(lambda x: remove_extra_spaces(x))\n",
    "df['C_retweet_tag'] = df['C_extra_spaces'].apply(lambda x: remove_retweet_tag(x))\n",
    "df['C_emojis'] = df['C_retweet_tag'].apply(lambda x: remove_emojis(x))\n",
    "\n",
    "df = df[['label', 'text', 'C_emojis']]\n",
    "df = df.rename(columns={'text': 'old_text', 'C_emojis': 'text'})\n",
    "df = df[['label', 'text']]\n",
    "df.head()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57ea303c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: snowballstemmer in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45274, 66866)\n",
      "Sparse Matrix :\n",
      "   (0, 59667)\t1\n",
      "  (0, 15073)\t1\n",
      "  (0, 63422)\t1\n",
      "  (0, 48463)\t1\n",
      "  (0, 63)\t1\n",
      "  (0, 9159)\t1\n",
      "  (0, 31283)\t1\n",
      "  (0, 2181)\t1\n",
      "  (0, 17996)\t1\n",
      "  (1, 9876)\t1\n",
      "  (1, 51263)\t1\n",
      "  (2, 31630)\t1\n",
      "  (2, 14481)\t1\n",
      "  (2, 51005)\t1\n",
      "  (2, 21989)\t1\n",
      "  (2, 35936)\t1\n",
      "  (2, 15219)\t1\n",
      "  (2, 23631)\t1\n",
      "  (2, 20369)\t1\n",
      "  (2, 12846)\t2\n",
      "  (2, 46696)\t1\n",
      "  (2, 31941)\t1\n",
      "  (2, 41820)\t1\n",
      "  (2, 11320)\t1\n",
      "  (2, 15396)\t1\n",
      "  :\t:\n",
      "  (45269, 63436)\t1\n",
      "  (45269, 46731)\t1\n",
      "  (45269, 41375)\t1\n",
      "  (45270, 7972)\t1\n",
      "  (45270, 4308)\t1\n",
      "  (45271, 47317)\t1\n",
      "  (45271, 32820)\t1\n",
      "  (45271, 41456)\t1\n",
      "  (45271, 21262)\t1\n",
      "  (45272, 44529)\t1\n",
      "  (45272, 54233)\t1\n",
      "  (45272, 20537)\t1\n",
      "  (45273, 40001)\t2\n",
      "  (45273, 35167)\t1\n",
      "  (45273, 34374)\t1\n",
      "  (45273, 62799)\t1\n",
      "  (45273, 23665)\t1\n",
      "  (45273, 15113)\t1\n",
      "  (45273, 40289)\t1\n",
      "  (45273, 37382)\t1\n",
      "  (45273, 17140)\t1\n",
      "  (45273, 46594)\t1\n",
      "  (45273, 63256)\t1\n",
      "  (45273, 62764)\t1\n",
      "  (45273, 159)\t1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 59667)\\t1\\n  (0, 15073)\\t1\\n  (0, 63422)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 9876)\\t1\\n  (0, 51263)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 31630)\\t1\\n  (0, 14481)\\t1\\n  (0, 51005)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 11206)\\t1\\n  (0, 15328)\\t1\\n  (0, 58930)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 14968)\\t1\\n  (0, 7345)\\t1\\n  (0, 41602)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45269</th>\n",
       "      <td>(0, 23727)\\t1\\n  (0, 11160)\\t1\\n  (0, 12841)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45270</th>\n",
       "      <td>(0, 7972)\\t1\\n  (0, 4308)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45271</th>\n",
       "      <td>(0, 47317)\\t1\\n  (0, 32820)\\t1\\n  (0, 41456)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45272</th>\n",
       "      <td>(0, 44529)\\t1\\n  (0, 54233)\\t1\\n  (0, 20537)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45273</th>\n",
       "      <td>(0, 40001)\\t2\\n  (0, 35167)\\t1\\n  (0, 34374)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45274 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0        (0, 59667)\\t1\\n  (0, 15073)\\t1\\n  (0, 63422)...\n",
       "1                          (0, 9876)\\t1\\n  (0, 51263)\\t1\n",
       "2        (0, 31630)\\t1\\n  (0, 14481)\\t1\\n  (0, 51005)...\n",
       "3        (0, 11206)\\t1\\n  (0, 15328)\\t1\\n  (0, 58930)...\n",
       "4        (0, 14968)\\t1\\n  (0, 7345)\\t1\\n  (0, 41602)\\...\n",
       "...                                                  ...\n",
       "45269    (0, 23727)\\t1\\n  (0, 11160)\\t1\\n  (0, 12841)...\n",
       "45270                       (0, 7972)\\t1\\n  (0, 4308)\\t1\n",
       "45271    (0, 47317)\\t1\\n  (0, 32820)\\t1\\n  (0, 41456)...\n",
       "45272    (0, 44529)\\t1\\n  (0, 54233)\\t1\\n  (0, 20537)\\t1\n",
       "45273    (0, 40001)\\t2\\n  (0, 35167)\\t1\\n  (0, 34374)...\n",
       "\n",
       "[45274 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "#punctuation\n",
    "import string\n",
    "string.punctuation\n",
    "def remove_punct(text):\n",
    "    \n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "df['nopunc'] = df['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "#2\n",
    "#tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "df['tokenized'] = df['nopunc'].apply(lambda x: tokenize(x))\n",
    "\n",
    "#3\n",
    "#stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_Ar = nltk.corpus.stopwords.words('arabic')\n",
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopwords_Ar]\n",
    "    return text\n",
    "\n",
    "df['nostop'] = df['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "#4\n",
    "#stemmer\n",
    "!pip install snowballstemmer\n",
    "from snowballstemmer import stemmer\n",
    "ar_stem = stemmer(\"arabic\")\n",
    "import nltk\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "df['stemmed'] = df['nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "### Create function to remove punctuation, tokenize, remove stopwords, and stem\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "     #tokens = re.split('\\W+', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords_Ar])\n",
    "    return text\n",
    "df=df[['label','text']]\n",
    "df['cleaned_text'] = df['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# To create a Count Vectorizer, we simply need to instantiate one.\n",
    "# There are special parameters we can set here when making the vectorizer, but\n",
    "# for the most basic example, it is not needed.\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "features_CountVec = vectorizer.fit_transform(df['cleaned_text'])\n",
    "print(features_CountVec.shape)\n",
    "print('Sparse Matrix :\\n', features_CountVec)\n",
    "#features_CountVec = pd.DataFrame(features_CountVec.toarray())\n",
    "features_CountVec = pd.DataFrame(features_CountVec)\n",
    "\n",
    "features_CountVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a29a417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "max_features = 10000\n",
    "embedding_dim = 100\n",
    "maxlen = 300\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "# Split the dataset into training and testing sets\n",
    "X = df['cleaned_text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Tokenize and pad the text sequences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca673896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce2f0091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 100)          1000000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 296, 256)          128256    \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 256)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,144,769\n",
      "Trainable params: 1,144,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1132/1132 [==============================] - 125s 109ms/step - loss: 0.5099 - accuracy: 0.7257 - auc: 0.8131 - val_loss: 0.4520 - val_accuracy: 0.7651 - val_auc: 0.8597\n",
      "Epoch 2/10\n",
      "1132/1132 [==============================] - 121s 107ms/step - loss: 0.3334 - accuracy: 0.8415 - auc: 0.9285 - val_loss: 0.4610 - val_accuracy: 0.7724 - val_auc: 0.8655\n",
      "Epoch 3/10\n",
      "1132/1132 [==============================] - 113s 100ms/step - loss: 0.2054 - accuracy: 0.9079 - auc: 0.9736 - val_loss: 0.5767 - val_accuracy: 0.7756 - val_auc: 0.8642\n",
      "Epoch 4/10\n",
      "1132/1132 [==============================] - 92s 81ms/step - loss: 0.1402 - accuracy: 0.9370 - auc: 0.9874 - val_loss: 0.7742 - val_accuracy: 0.7660 - val_auc: 0.8508\n",
      "Epoch 5/10\n",
      "1132/1132 [==============================] - 92s 81ms/step - loss: 0.1119 - accuracy: 0.9472 - auc: 0.9917 - val_loss: 0.8774 - val_accuracy: 0.7690 - val_auc: 0.8477\n",
      "Epoch 6/10\n",
      "1132/1132 [==============================] - 91s 81ms/step - loss: 0.0968 - accuracy: 0.9538 - auc: 0.9936 - val_loss: 1.1059 - val_accuracy: 0.7738 - val_auc: 0.8382\n",
      "Epoch 7/10\n",
      "1132/1132 [==============================] - 95s 84ms/step - loss: 0.0905 - accuracy: 0.9563 - auc: 0.9943 - val_loss: 1.3081 - val_accuracy: 0.7637 - val_auc: 0.8301\n",
      "Epoch 8/10\n",
      "1132/1132 [==============================] - 99s 88ms/step - loss: 0.0845 - accuracy: 0.9585 - auc: 0.9950 - val_loss: 1.3106 - val_accuracy: 0.7592 - val_auc: 0.8280\n",
      "Epoch 9/10\n",
      "1132/1132 [==============================] - 91s 80ms/step - loss: 0.0809 - accuracy: 0.9598 - auc: 0.9953 - val_loss: 1.3752 - val_accuracy: 0.7664 - val_auc: 0.8309\n",
      "Epoch 10/10\n",
      "1132/1132 [==============================] - 92s 81ms/step - loss: 0.0778 - accuracy: 0.9599 - auc: 0.9956 - val_loss: 1.3893 - val_accuracy: 0.7664 - val_auc: 0.8293\n",
      "283/283 [==============================] - 6s 20ms/step - loss: 1.3893 - accuracy: 0.7664 - auc: 0.8293\n",
      "Model\tAccuracy\tLoss\tAUC\n",
      "CNN\t0.77\t1.39\t0.83\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(Conv1D(256, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy, auc = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN\\t{accuracy:.2f}\\t{loss:.2f}\\t{auc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e63dc97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33c364eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 300, 100)          1000000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,125,569\n",
      "Trainable params: 1,125,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1132/1132 [==============================] - 537s 470ms/step - loss: 0.5111 - accuracy: 0.7248 - auc_1: 0.8116 - val_loss: 0.4609 - val_accuracy: 0.7563 - val_auc_1: 0.8523\n",
      "Epoch 2/10\n",
      "1132/1132 [==============================] - 691s 611ms/step - loss: 0.3752 - accuracy: 0.8169 - auc_1: 0.9066 - val_loss: 0.4618 - val_accuracy: 0.7611 - val_auc_1: 0.8536\n",
      "Epoch 3/10\n",
      "1132/1132 [==============================] - 526s 465ms/step - loss: 0.2995 - accuracy: 0.8560 - auc_1: 0.9411 - val_loss: 0.5049 - val_accuracy: 0.7676 - val_auc_1: 0.8567\n",
      "Epoch 4/10\n",
      "1132/1132 [==============================] - 529s 467ms/step - loss: 0.2453 - accuracy: 0.8815 - auc_1: 0.9605 - val_loss: 0.5996 - val_accuracy: 0.7699 - val_auc_1: 0.8531\n",
      "Epoch 5/10\n",
      "1132/1132 [==============================] - 483s 427ms/step - loss: 0.2080 - accuracy: 0.8987 - auc_1: 0.9716 - val_loss: 0.7467 - val_accuracy: 0.7679 - val_auc_1: 0.8453\n",
      "Epoch 6/10\n",
      "1132/1132 [==============================] - 468s 414ms/step - loss: 0.1811 - accuracy: 0.9105 - auc_1: 0.9783 - val_loss: 0.8295 - val_accuracy: 0.7649 - val_auc_1: 0.8421\n",
      "Epoch 7/10\n",
      "1132/1132 [==============================] - 473s 418ms/step - loss: 0.1603 - accuracy: 0.9209 - auc_1: 0.9829 - val_loss: 0.8872 - val_accuracy: 0.7674 - val_auc_1: 0.8405\n",
      "Epoch 8/10\n",
      "1132/1132 [==============================] - 455s 402ms/step - loss: 0.1440 - accuracy: 0.9299 - auc_1: 0.9861 - val_loss: 0.9608 - val_accuracy: 0.7686 - val_auc_1: 0.8370\n",
      "Epoch 9/10\n",
      "1132/1132 [==============================] - 454s 401ms/step - loss: 0.1292 - accuracy: 0.9370 - auc_1: 0.9887 - val_loss: 1.0882 - val_accuracy: 0.7686 - val_auc_1: 0.8332\n",
      "Epoch 10/10\n",
      "1132/1132 [==============================] - 458s 405ms/step - loss: 0.1191 - accuracy: 0.9407 - auc_1: 0.9904 - val_loss: 1.2608 - val_accuracy: 0.7634 - val_auc_1: 0.8264\n",
      "283/283 [==============================] - 17s 61ms/step - loss: 1.2608 - accuracy: 0.7634 - auc_1: 0.8264\n",
      "Model\tAccuracy\tLoss\tAUC\n",
      "LSTM\t0.76\t1.26\t0.83\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set and print the results\n",
    "loss, accuracy, auc = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'LSTM\\t{accuracy:.2f}\\t{loss:.2f}\\t{auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4ad7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df84bd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 300, 100)          1000000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              84480     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,092,801\n",
      "Trainable params: 1,092,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1132/1132 [==============================] - 459s 398ms/step - loss: 0.5106 - accuracy: 0.7246 - auc_2: 0.8129 - val_loss: 0.4665 - val_accuracy: 0.7523 - val_auc_2: 0.8508\n",
      "Epoch 2/10\n",
      "1132/1132 [==============================] - 436s 385ms/step - loss: 0.3861 - accuracy: 0.8083 - auc_2: 0.8999 - val_loss: 0.4758 - val_accuracy: 0.7605 - val_auc_2: 0.8538\n",
      "Epoch 3/10\n",
      "1132/1132 [==============================] - 434s 383ms/step - loss: 0.3148 - accuracy: 0.8462 - auc_2: 0.9342 - val_loss: 0.5287 - val_accuracy: 0.7674 - val_auc_2: 0.8552\n",
      "Epoch 4/10\n",
      "1132/1132 [==============================] - 444s 392ms/step - loss: 0.2649 - accuracy: 0.8665 - auc_2: 0.9529 - val_loss: 0.5975 - val_accuracy: 0.7664 - val_auc_2: 0.8504\n",
      "Epoch 5/10\n",
      "1132/1132 [==============================] - 423s 374ms/step - loss: 0.2289 - accuracy: 0.8819 - auc_2: 0.9641 - val_loss: 0.6958 - val_accuracy: 0.7661 - val_auc_2: 0.8444\n",
      "Epoch 6/10\n",
      "1132/1132 [==============================] - 419s 371ms/step - loss: 0.2035 - accuracy: 0.8936 - auc_2: 0.9713 - val_loss: 0.8530 - val_accuracy: 0.7687 - val_auc_2: 0.8426\n",
      "Epoch 7/10\n",
      "1132/1132 [==============================] - 579s 512ms/step - loss: 0.1823 - accuracy: 0.9050 - auc_2: 0.9772 - val_loss: 0.9325 - val_accuracy: 0.7636 - val_auc_2: 0.8380\n",
      "Epoch 8/10\n",
      "1132/1132 [==============================] - 818s 722ms/step - loss: 0.1660 - accuracy: 0.9153 - auc_2: 0.9812 - val_loss: 1.0032 - val_accuracy: 0.7707 - val_auc_2: 0.8374\n",
      "Epoch 9/10\n",
      "1132/1132 [==============================] - 826s 730ms/step - loss: 0.1515 - accuracy: 0.9229 - auc_2: 0.9843 - val_loss: 1.0677 - val_accuracy: 0.7637 - val_auc_2: 0.8332\n",
      "Epoch 10/10\n",
      "1132/1132 [==============================] - 822s 726ms/step - loss: 0.1375 - accuracy: 0.9305 - auc_2: 0.9870 - val_loss: 1.2960 - val_accuracy: 0.7687 - val_auc_2: 0.8278\n",
      "283/283 [==============================] - 23s 80ms/step - loss: 1.2960 - accuracy: 0.7687 - auc_2: 0.8278\n",
      "Model\tAccuracy\tLoss\tAUC\n",
      "BiLSTM\t0.768747\t1.296031\t0.827837\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build the BiLSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy, auc = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'BiLSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f82e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a372c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 300, 100)          1000000   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 298, 128)          38528     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 149, 128)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 149, 128)          0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,092,161\n",
      "Trainable params: 1,092,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1132/1132 [==============================] - 245s 211ms/step - loss: 0.5092 - accuracy: 0.7251 - auc_3: 0.8127 - val_loss: 0.4585 - val_accuracy: 0.7625 - val_auc_3: 0.8553\n",
      "Epoch 2/10\n",
      "1132/1132 [==============================] - 227s 200ms/step - loss: 0.3558 - accuracy: 0.8289 - auc_3: 0.9173 - val_loss: 0.4656 - val_accuracy: 0.7679 - val_auc_3: 0.8625\n",
      "Epoch 3/10\n",
      "1132/1132 [==============================] - 225s 199ms/step - loss: 0.2426 - accuracy: 0.8903 - auc_3: 0.9629 - val_loss: 0.5616 - val_accuracy: 0.7707 - val_auc_3: 0.8580\n",
      "Epoch 4/10\n",
      "1132/1132 [==============================] - 227s 201ms/step - loss: 0.1712 - accuracy: 0.9232 - auc_3: 0.9813 - val_loss: 0.7904 - val_accuracy: 0.7663 - val_auc_3: 0.8456\n",
      "Epoch 5/10\n",
      "1132/1132 [==============================] - 215s 190ms/step - loss: 0.1340 - accuracy: 0.9402 - auc_3: 0.9886 - val_loss: 0.9520 - val_accuracy: 0.7650 - val_auc_3: 0.8413\n",
      "Epoch 6/10\n",
      "1132/1132 [==============================] - 229s 203ms/step - loss: 0.1119 - accuracy: 0.9483 - auc_3: 0.9916 - val_loss: 1.0569 - val_accuracy: 0.7634 - val_auc_3: 0.8342\n",
      "Epoch 7/10\n",
      "1132/1132 [==============================] - 222s 196ms/step - loss: 0.1004 - accuracy: 0.9531 - auc_3: 0.9932 - val_loss: 1.1498 - val_accuracy: 0.7679 - val_auc_3: 0.8309\n",
      "Epoch 8/10\n",
      "1132/1132 [==============================] - 220s 195ms/step - loss: 0.0926 - accuracy: 0.9553 - auc_3: 0.9942 - val_loss: 1.3584 - val_accuracy: 0.7644 - val_auc_3: 0.8266\n",
      "Epoch 9/10\n",
      "1132/1132 [==============================] - 172s 152ms/step - loss: 0.0889 - accuracy: 0.9569 - auc_3: 0.9946 - val_loss: 1.3083 - val_accuracy: 0.7706 - val_auc_3: 0.8290\n",
      "Epoch 10/10\n",
      "1132/1132 [==============================] - 132s 117ms/step - loss: 0.0838 - accuracy: 0.9581 - auc_3: 0.9951 - val_loss: 1.3821 - val_accuracy: 0.7693 - val_auc_3: 0.8265\n",
      "283/283 [==============================] - 10s 35ms/step - loss: 1.3821 - accuracy: 0.7693 - auc_3: 0.8265\n",
      "Model\tAccuracy\tLoss\tAUC\n",
      "CNN + LSTM\t0.769299\t1.382125\t0.826522\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build the CNN+LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy, auc = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN + LSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13c2e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN+BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65f1324b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 300, 100)          1000000   \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 298, 128)          38528     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 149, 128)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 149, 128)          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,145,665\n",
      "Trainable params: 1,145,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1132/1132 [==============================] - 163s 139ms/step - loss: 0.5123 - accuracy: 0.7247 - auc_4: 0.8111 - val_loss: 0.4590 - val_accuracy: 0.7618 - val_auc_4: 0.8529\n",
      "Epoch 2/10\n",
      "1132/1132 [==============================] - 152s 134ms/step - loss: 0.3553 - accuracy: 0.8306 - auc_4: 0.9172 - val_loss: 0.4528 - val_accuracy: 0.7658 - val_auc_4: 0.8622\n",
      "Epoch 3/10\n",
      "1132/1132 [==============================] - 154s 136ms/step - loss: 0.2435 - accuracy: 0.8887 - auc_4: 0.9625 - val_loss: 0.5650 - val_accuracy: 0.7651 - val_auc_4: 0.8574\n",
      "Epoch 4/10\n",
      "1132/1132 [==============================] - 160s 141ms/step - loss: 0.1699 - accuracy: 0.9224 - auc_4: 0.9817 - val_loss: 0.7721 - val_accuracy: 0.7643 - val_auc_4: 0.8449\n",
      "Epoch 5/10\n",
      "1132/1132 [==============================] - 154s 136ms/step - loss: 0.1333 - accuracy: 0.9398 - auc_4: 0.9885 - val_loss: 0.8267 - val_accuracy: 0.7612 - val_auc_4: 0.8402\n",
      "Epoch 6/10\n",
      "1132/1132 [==============================] - 154s 136ms/step - loss: 0.1114 - accuracy: 0.9481 - auc_4: 0.9919 - val_loss: 1.1282 - val_accuracy: 0.7585 - val_auc_4: 0.8291\n",
      "Epoch 7/10\n",
      "1132/1132 [==============================] - 155s 137ms/step - loss: 0.1009 - accuracy: 0.9523 - auc_4: 0.9931 - val_loss: 1.2513 - val_accuracy: 0.7669 - val_auc_4: 0.8300\n",
      "Epoch 8/10\n",
      "1132/1132 [==============================] - 153s 135ms/step - loss: 0.0931 - accuracy: 0.9562 - auc_4: 0.9940 - val_loss: 1.3280 - val_accuracy: 0.7631 - val_auc_4: 0.8239\n",
      "Epoch 9/10\n",
      "1132/1132 [==============================] - 152s 134ms/step - loss: 0.0879 - accuracy: 0.9565 - auc_4: 0.9946 - val_loss: 1.1906 - val_accuracy: 0.7623 - val_auc_4: 0.8287\n",
      "Epoch 10/10\n",
      "1132/1132 [==============================] - 152s 135ms/step - loss: 0.0853 - accuracy: 0.9584 - auc_4: 0.9950 - val_loss: 1.4036 - val_accuracy: 0.7633 - val_auc_4: 0.8222\n",
      "283/283 [==============================] - 10s 37ms/step - loss: 1.4036 - accuracy: 0.7633 - auc_4: 0.8222\n",
      "Model\tAccuracy\tLoss\tAUC\n",
      "CNN + BiLSTM\t0.763335\t1.403632\t0.822176\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build the CNN+BiLSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy, auc = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN + BiLSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdcaad73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# CNN\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loss, accuracy, auc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cnn\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(X_test_pad, y_test)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_cnn' is not defined"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "loss, accuracy, auc = model_cnn.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')\n",
    "\n",
    "# LSTM\n",
    "loss, accuracy, auc = model_lstm.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'LSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')\n",
    "\n",
    "# CNN + LSTM\n",
    "loss, accuracy, auc = model_cnn_lstm.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN + LSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')\n",
    "\n",
    "# BiLSTM\n",
    "loss, accuracy, auc = model_bilstm.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'BiLSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')\n",
    "\n",
    "# CNN + BiLSTM\n",
    "loss, accuracy, auc = model_cnn_bilstm.evaluate(X_test_pad, y_test)\n",
    "print(f'Model\\tAccuracy\\tLoss\\tAUC')\n",
    "print(f'CNN + BiLSTM\\t{accuracy:.6f}\\t{loss:.6f}\\t{auc:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f502fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
